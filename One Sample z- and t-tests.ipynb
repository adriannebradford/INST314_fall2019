{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Sample t-tests\n",
    "One sample t-tests are used to compare the mean of some data with a known value.  Because it is unlikely that you will be comparing your sample data with a known population mean that has a known population standard error, we will focus on t- and not z-tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(DescTools)\n",
    "library(plotrix)\n",
    "library(lsr)\n",
    "library(pwr)\n",
    "library(readxl)\n",
    "\n",
    "options(repr.plot.width=5, repr.plot.height=5) ## set options for plot size within the notebook -\n",
    "# this is only for jupyter notebooks, you can disregard this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the mtcars dataset for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(mtcars)  ## remind ourselves of the setup of the mtcars dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Sample t-test\n",
    "We want to know if the average mpg of the cars in our sample `mtcars` is lower than the current national average of 24.7 mpg.\n",
    "\n",
    "How would we run a hypothesis test for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Formulate Hypothesis\n",
    "\n",
    "Population mean is $\\mu$, which specified in our question - 24.7mpg\n",
    "\n",
    "Sample mean is $\\bar{x}$ which we will calculate from our data.\n",
    "\n",
    "$H_0 : \\bar{x} = 24.7$\n",
    "\n",
    "$H_A : \\bar{x} < 24.7$\n",
    "\n",
    "**Note:** Given our $H_A$ we're running a one-tailed test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Prepare and Check Conditions\n",
    "\n",
    "Set alpha ->>> $\\alpha = 0.05$\n",
    "\n",
    "Random and independent sample ->>> assumed for now\n",
    "\n",
    "Sample is <10% of the population? ->>> Yes\n",
    "\n",
    "Let's take a look at our distribution and summary statistics first to get an idea of our sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IS OUR SAMPLE DISTRIBUTION NORMAL?\n",
    "qqnorm(mtcars$mpg) ## calculate QQ values\n",
    "qqline(mtcars$mpg, col=\"red\")  ## create QQ plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(mtcars$mpg) ## summary statistics - mean, median, min, max, etc.\n",
    "sd(mtcars$mpg) ## standard deviation\n",
    "std.error(mtcars$mpg) ## standard error\n",
    "\n",
    "## show that the standard error is sd/sqrt(n)\n",
    "sd(mtcars$mpg)/sqrt(length(mtcars$mpg))\n",
    "\n",
    "## what is our n?\n",
    "length(mtcars$mpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sample data deviates from normality a bit in the tails, but it's actually not that bad in terms of approximating a normal distribution.\n",
    "\n",
    "So we can already see that our sample mean is 20.09, which is lower than 24.7, but is the difference statistically significant?\n",
    "\n",
    "### Step 3 - Calculate t-statistic and p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using t.test to calculate one-sample t-statistic - t.test(VECTOR, mu, alternative = \"one.sided\" or\" less\" or \"greater\",\n",
    "                                                                                                # conf= 1- alpha)\n",
    "\n",
    "t.test(mtcars$mpg, mu = 24.7, alternative = \"less\", conf = 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So what we see in the output is:**\n",
    "\n",
    "our t-statistic/t-value is -4.3263\n",
    "\n",
    "we have 31 degrees of freedom (n-1)\n",
    "\n",
    "It tells us in words what our H_A is\n",
    "\n",
    "and it gives us a 95% CI.  This is the 95% CI related to the \"less than\" hypothesis, therefore it starts at -Inf, because we're looking at that side of the distribution.  \n",
    "\n",
    "If we wanted a 95% CI as a range, we'd run a two-sided test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using t.test to calculate one-sample t-statistic - t.test(VECTOR, mu, alternative = \"one.sided\" or\" less\" or \"greater\",\n",
    "                                                                                                # conf= 1- alpha)\n",
    "\n",
    "t.test(mtcars$mpg, mu = 24.7, alternative = \"two.sided\", conf = 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a 95% CI centered around our sample mean, but it also is testing a different alternative hypothesis, whether the mean is different from 24.7 in any direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect Size\n",
    "\n",
    "We now want to know how big the difference is between our sample mean and the population mean.  For this we will calculate Cohen's d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohensD(mtcars$mpg, 24.7) ## cohen's d function, two arguments, our vector of data, and our population mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our \"rule of thumb\" for interpreting Cohen's d is:\n",
    "\n",
    "| Cohen's d | Effect Size |\n",
    "|:---------:|:-----------:|\n",
    "| 0.20 | Small |\n",
    "| 0.50 | Medium|\n",
    "| 0.80 | Large |\n",
    "\n",
    "So our value of 0.765 is consistent with a relatively large effect size (e.g. difference between sample mean and population mean).  Would you consider a difference in about 4.7 mpg a difference that would be important in the real world?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power\n",
    "\n",
    "Finally, we want to see how much power our test has.  To conduct a power analysis we need 5 of the 6 pieces of information:\n",
    "\n",
    "- n: sample size\n",
    "- d: effect size (in this case cohensD)\n",
    "- sig.level: alpha\n",
    "- power: power (1-$\\beta$)\n",
    "- alternative: type of alternative hypothesis (two.sided, less, or greater)\n",
    "- type: right now we're only looking at one.sample tests\n",
    "\n",
    "When we run the power function we set one of these things to NULL, to calculate that value based on the other inputs.  Typically we will either be wanting to calculate a sample size *a priori* or to calculate the power of an analysis after you conduct the analysis.\n",
    "\n",
    "We want to get the power for our mpg analysis, so we know our arguments will be:\n",
    "\n",
    "- n = 32\n",
    "- d = 0.765\n",
    "- sig.level = 0.05\n",
    "- power = NULL\n",
    "- alternative = \"two-sided\" - Let's look at the more \"conservative\" version of the test.\n",
    "- type = \"one.sample\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x <- pwr.t.test(d = 0.765, n = 32, sig.level=0.05, power = NULL, alternative = \"two.sided\", type = \"one.sample\")\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our power is extremely high.  If we only wanted to have a power of 0.80 we wouldn't have needed as large of a sample, given our effect size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwr.t.test(d = 0.765, n = NULL, sig.level=0.05, power = 0.80, alternative = \"two.sided\", type = \"one.sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Sample t-test - Proportions\n",
    "\n",
    "At the most basic level a proportion is a mean of a vector of 0/1 observations.  Given this, we can also do t-tests to calculate the difference in proportions.\n",
    "\n",
    "For this we'll use the small_gss dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_gss <- read_xls(\"small_gss.xls\")\n",
    "head(small_gss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at the proportion of female respondents who report that they support abortion for any reason `abany`.  We'll compare our sample mean to the value of 61% support as reported by Pew - https://www.pewforum.org/fact-sheet/public-opinion-on-abortion/\n",
    "\n",
    "Before we can conduct our analysis I'm going to do some quick data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_gss  %>% count(abany) #pre-recoding counts\n",
    "df <- small_gss %>% ## save recoded version as df\n",
    "    filter (abany != \"Not applicable\")  %>% #filter out rows where abany was missing (because the question wasn't asked)\n",
    "    filter (sex == \"Female\")  %>%    ## we've defined our population as women only, so we'll subset to just those observations\n",
    "    mutate (abany = ifelse(abany == \"Yes\", 1, 0)) ## using ifelse to convert chr yes/no to numerical 1/0 \n",
    "                                                ## the format of the function is ifelse(test, valiftrue, valiffalse)\n",
    "df %>%count(abany)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Formulate Hypothesis\n",
    "\n",
    "Population proportion is $p_u$, which specified in our question - 0.61 (61%)\n",
    "\n",
    "Sample proportion is $p_s$ which we will calculate from our data.\n",
    "\n",
    "$H_0 : p_s = 0.61$\n",
    "\n",
    "$H_A : p_s \\neq 0.61$\n",
    "\n",
    "Given our $H_A$ we're running a two-tailed test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Prepare and Check Conditions\n",
    "\n",
    "Set alpha ->>> $\\alpha = 0.05$\n",
    "\n",
    "Random and independent sample ->>> assumed for now\n",
    "\n",
    "Sample is <10% of the population? ->>> Yes\n",
    "\n",
    "Let's take a look at our distribution and summary statistics first to get an idea of our sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(df$abany) ## mean of 0/1 variable is proportion saying \"yes\"\n",
    "p <- mean(df$abany) ## save prop to use below\n",
    "\n",
    "std.error(df$abany) ## standard error\n",
    "## show that the standard error is square root of p_s(1-p_s)/n\n",
    "sqrt(p*(1-p)/length(df$abany))\n",
    "\n",
    "## what is our n?\n",
    "length(df$abany)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can already see that our sample proportion is 0.47, which seems much different from 0.61, but is the difference statistically significant?\n",
    "\n",
    "(SPOILER ALERT: it has to be, like really...)\n",
    "\n",
    "### Step 3 - Calculate t-statistic and p-value\n",
    "\n",
    "The correct way to compare a one-sample proportion with a given probability to test against, is by using `prop.test()`.\n",
    "\n",
    "#### prop.test()\n",
    "`prop.test()` takes 5 arguments:\n",
    "\n",
    "- x = the total number of \"yes\" or \"success\" or ones\n",
    "- n = the total number of observations\n",
    "- p = the probability to test against (default is 0.5)\n",
    "- alternative = the type of test to conduct (\"two.sided\", \"less\", or \"greater\") depending on your alternative hypothesis\n",
    "- correct = optional to use Yates' continuity correction - default is TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prop.test(x, n, p = NULL, alternative = \"two.sided\", correct = TRUE)\n",
    "x = sum(df$abany) ## summing a 0/1 variable gives you the count of \"yesses\"\n",
    "n = length(df$abany) ## length of a vector gives you the number of values in that vector\n",
    "p = 0.61 ## the probability we want to test against\n",
    "prop.test(x, n, p = p, alternative = \"two.sided\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output shows a chi-squared value, not a t-value!  That's because under the hood R is calculating this using the chi-square distribution because we're looking at frequencies (counts).  Our extremely low p-value indicates that we can reject the null hypothesis (no surprise).\n",
    "\n",
    "#### t.test() with 0/1 data\n",
    "\n",
    "Given that our proportion is a mean of a 0/1 variable, we can simply use our t-test function and obtain similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_t <- t.test(df$abany, mu = 0.61, alternative = \"two.sided\", conf = 0.95) ### remember we can save our results\n",
    "prop_t ## and then print them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see almost identical results, but this time we have a t-value and our alternative hypothesis references a mean of 0.61.\n",
    "\n",
    "In that block I saved the results to an object called `prop_t`.  Remember when we did the chi-square test that that object held all of the information outputted from the test.  Let's look at the structure of that object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(prop_t) ## look at the structure of the t.test output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get all of the \"pieces\" of the result as parts of the object - it's a list where we have key:value pairs.  We can use those keys to obtain the single pieces of information we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_t$statistic # t value/statistic\n",
    "prop_t$parameter # degrees of freedom\n",
    "prop_t$conf.int # confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect Size - Comparing Proportions\n",
    "\n",
    "To compare our proportions, we should look at Cohen's H, which is a non-directional magnitude of difference in proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The function is ES.h(ps,pu) - remember ps is sample prop. and pu is population (universe) prop.\n",
    "\n",
    "ES.h(mean(df$abany),0.61)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohensD(df$abany, 0.61)  ## try the Cohen's D, which is the mean effect size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculated Cohen's H, for proportions, and also Cohen's D to compare (given that the mean of 0/1 data is a proportion).  We got very similar results in terms of absolute magnitude.  The negative sign for the Cohen's H result is because it's a uni-directional test.  However I wouldn't rely on using Cohen's D for proportion data unless you find that they are virtually identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power\n",
    "\n",
    "We can also calculate the power of a comparison of proportions.  All of the arguments are similar to those above, except notice we're using a different formula (pwr.<b>p</b>.test, not pwr.<b>t</b>.test) and we have <b>h</b> as the effect size, not <b>d</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 <- ES.h(mean(df$abany), 0.61)\n",
    "n1 <- length(df$abany)\n",
    "\n",
    "\n",
    "pwr.p.test(h = h1, n= n1, sig.level=0.05, power=NULL, alternative=\"two.sided\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our difference is so big that our power is virtually 1.  Which means we have pretty much no probability of Type II error.  Let's compare to using the `pwr.t.test()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 <- cohensD(df$abany, 0.61)\n",
    "\n",
    "pwr.t.test(d = d1, \n",
    "           n = n1, ## same n1 we set in the block above\n",
    "           sig.level=0.05, \n",
    "           power = NULL, \n",
    "           alternative = \"two.sided\", \n",
    "           type = \"one.sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get essentially the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS CONTENT: Power Plots\n",
    "\n",
    "Power plots show us the range of sample sizes for a range of levels of power.  \"Optimal\" sample size line is the number of observations you would need to achieve the power specified in the function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_plot_data <- pwr.t.test(d=0.30, n=NULL, sig.level=0.05, power = 0.8, alternative = \"two.sided\")\n",
    "plot(power_plot_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
